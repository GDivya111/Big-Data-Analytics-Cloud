According to our understanding, the basic use case scenario is as
follows:

First, the user logs in to the system. He injects a database on which
data analysis needs to be done.He also mentions the time frame within
which data analysis should be done.

Now the database provided by the user will be first brought into
HDFS. Multiple compute nodes (number of compute nodes will be decided
based on the time frame provided by the user - autoscaling) will be
spawned using OpenStack and each of those compute node will behave as
a hadoop cluster node (This will be done by using Ambari). All these
nodes will run the Apache Solr platform for indexing the data stored
in HDFS (The indexed data will be stored in HDFS). Once data
indexing is done, these same compute nodes will start running
HortonWorks data analysis engine to process the indexed data.

Below are the doubts we have
1. How does the user interact with the system? - Via Ambari interface?
Should he be allowed to login to Horizon?
2. Where will HDFS be installed? - SAN attached to cluster?
3. What data is provided by the user? - A postgreSQL database? Should
we directly import that data to HDFS?
4. What functionality is exactly provided by the Hortonwork package?
Does it need indexed data to provide that functionality?
5. what should be the user interface to view this processed data?







===================<Notes>==============================

Our document provides analysis of
1. SQL vs NoSQL
2. Performance impact over varying Compute nodes
3. what is a threshold time anyway? but apparently vary that too

Discussion:

3 important parts of data processing:
capture - nutch(crawler),  tika (data conversion), tessaract
analyze - solr, lucence, uima, R, tensorlfow, numpy
expose results - Django, flask, postgres

This all sits on top of postgres, H-Base

Capture -> H-base -> analyze -> PostGreSQL -> exposed result

Capture - How big is my network pipe, how much time do I have?,
	response time from others, my network architecture. Either
	user can ask to run crawler to capture data or he can inject
	his own database.

Analyze - MAx time required for indexing - this will decide the number
	of machines required

Expose results - how many concurrent users do I have?

Document everything - the hardware platform, the architecture
Hortonwroks Data platform(HDP) package (https://hortonworks.com/products/data-center/hdp/)


---------March 16-------
***Questions***
1. Should we include OpenStack installation in Functional Requirement (is it a functional requirement or just a design choice)
2. Which particular package of Hortonworks to be used ?
3. How will the user interact with system ?
(will the user interact with OpenStack OR Hortonwors OR Solr/Lucene OR do we need to provide a seperate UI for user to interact with system)
4. As per our understanding the options provided to user after logging into system will be:
   -the option to inject database (?from where will exactly the database will be injected....will it be injected as Cinder Volume ?)
   - can user add multiple databases? (eg. some files of MySQL db, some of xml, some of json)...then in this case user must tell which database it is adding so that Solr/Lucene can index it in that specific way
   - do we really need HDFS ? (if yes, then how and why ?)
   - where to store the indexed data generated by Solr
   

***Functional Requirements***
   1. User should be able to login to system and should be able to inject his database (SQL/Non-SQL)
   2. Apache Solr should be used for indexing this injected dataset. The job of indexing will be done by minimum 2 Virtual Machines. However, user should be able to specify the time limit for this task which will decide the number of additional VM's to be created for this task.
   3. Further this indexed dataset will be used by Hortonworks analysis package for analyzing the data. Based on the time limit specified by the user, VM's will be created for performing this task and minimum of 2 VM's will be used.
   4. User should be able to view the cluster details during the Data Analysis phase and alter other configuration parameters.
